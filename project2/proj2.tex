\documentclass[a4paper,10pt]{article}

\usepackage{Nikolai}
\usepackage[margin=0.75in]{geometry}

\title{Scientific Computing Project 2}
\author{Nikolai Plambech Nielsen\\LPK331}
\date{\today}


\begin{document}
	\maketitle
	\section*{A}
	The code is seen in file \texttt{f2.py}. Using the function on $ \textbf{K} $ gives the Gershgorin disks seen in table \ref{tab:disks}
	\begin{table}[H]
		\centering
		\begin{tabular}{c|c}
			Center & Radius \\
			\hline
			22345.91813211761 & 90954.38918993676 \\
			20970.291359471106 & 111324.65280062004\\
			42687.082433334916 & 126764.0694086381\\
			26259.27613697939 & 94137.99241796896\\
			15959.10794172863 & 99958.28412923093\\
			53149.927128462994 & 171091.65853598804\\
			42173.70083943926 & 71865.46384544748\\
			31276.609541102425 & 124290.7338304566\\
			16126.984973178549 & 79552.99761063268\\
			39042.18505838955 & 129453.08357102858\\
			31503.391029932365 & 133585.4244613245\\
			18523.82363971299 & 82107.1199063304\\
			62872.789483587374 & 206937.40894591622\\
			19011.945981070516 & 76188.8354648638\\
			31881.247846857976 & 123877.63470683568
		\end{tabular}
	\caption{The Gershgorin disks for the matrix $ \textbf{K} $.}
	\label{tab:disks}
	\end{table}
	
	\section*{B}
	The convergence criterion chosen for the power iteration is
	\begin{equation}\label{key}
		\text{abs}\pp{\frac{\lambda_{n} - \lambda_{n-1}}{\lambda_{n-1}}} < \varepsilon
	\end{equation}
	where by default $ \varepsilon = 1\D 10^{-6} $, and the eigenvalue $ \lambda $ is calculated with the Rayleigh quotient. The eigenvalues, number of iterations before convergence and the Rayleigh residual of the example matrices and $ \V{K} $ are tabulated below in table \ref{tab:power_iter}
	\begin{table}[H]
		\centering
		\begin{tabular}{c|c|c|c}
			Matrix & Eigenvalue & Rayleigh Residual & Iterations \\
			\hline
			$ \textbf{A}_1 $ & 3.9999989184158142 & 0.0036011104009171287 & 10\\
			$ \textbf{A}_2 $ & 3.9999987743348386 & 0.0022124632616849476 & 7\\
			$ \textbf{A}_3 $ & 12.298959795018773 & 8.933595023604699e-06 & 14\\
			$ \textbf{A}_4 $ & 16.116844235105 & 1.139976430233264e-06 & 6\\
			$ \textbf{A}_5 $ & 68.64208108540393 & 1.2957189457185429e-06 & 6\\
			$ \textbf{A}_6 $ & 1.9999994028072163 & 0.001144211257731418 & 4\\
			$ \V{K} $ & 151362.6666519405 & 0.017048828321719074 & 30
		\end{tabular}
		\caption{Largest eigenvalue of the example matrices and $ \V{K} $ using power iteration, with Rayleigh residual and number of iterations until convergence shown.}
		\label{tab:power_iter}
	\end{table}
	For each of the tables a random starting vector is chosen with \texttt{np.random.uniform}
	
	\section*{C}
	The same convergence criterion is used for Rayleigh iteration as for power iteration. If we use the LU-solver for computing the solution, the method will not be robust for singular matrices. The shifts may make the matrix non-singular, allowing us to use the algorithm, but it probably should not be relied upon. As such we choose the QR-solver. The result are shown in table \ref{tab:rayleigh_iter}.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{c|c|c|c}
			Matrix & Eigenvalue & Rayleigh Residual & Iterations \\
			\hline
			$ \textbf{A}_1 $ & 4.0 &1.4505701123219478e-09 & 3\\
			$ \textbf{A}_2 $ & 4.0 &2.8805418676599316e-09 & 2\\
			$ \textbf{A}_3 $ & 12.29895839097071 & 1.1234667099445444e-14 & 5\\
			$ \textbf{A}_4 $ & 16.116843969809032 & 3.156822789041486e-11 & 4\\
			$ \textbf{A}_5 $ & 68.64208073700239 & 1.7112135109858138e-13 & 10\\
			$ \textbf{A}_6 $ & 2.0000000000000004 & 5.103831345936687e-11 & 2
		\end{tabular}
		\caption{Largest eigenvalue of the example matrices using Rayleigh iteration, with Rayleigh residual and number of iterations until convergence also shown.}
		\label{tab:rayleigh_iter}
	\end{table}
	
	
	\section*{D}
	To calculate multiple eigenvalues of $ \V{K} $ we need to use one or more of the algorithms above several times, with different shifts. In general the Rayleigh iteration uses fewer iterations before convergence is achieved, so this is the employed algorithm. We further use 5 inverse iterations with a constant shift to hone in on a specific eigenvalue, to counteract the fluctuations introduced by using a random starting vector.
	
	Next we localize the eigenvalues of $ \V{K} $ as done in problem A. For the first attempt at finding all eigenvalues, the start, middle and end of all intervals was used as starting shifts, but this yielded between 3 and 7 eigenvalues, depending on the seed.
	
	Due to this, we instead create 10 linearly spaced points for each of the intervals (the smallest value which would find 15 distinct eigenvalues consistently). Each of these 150 values are then used as a starting shift to generate a list of approximate eigenvalues and eigenvectors
	
	Then we just need to weed out the eigenvalues which are not unique, which should then give us all the eigenvalues with different values. The problem is that using strict numerical equality to evaluate uniqueness necessarily ignores any truncation error of the algorithm. To account for this we use the function \texttt{np.isclose} with a relative tolerance of $ 10^{-5} $ and an absolute tolerance of $ 10^{-8} $.
	
	The code (\texttt{find\_unique} in the file \texttt{f2.py}) runs a loop where the first entry is chosen, and used as the target of the isclose function. The ``true'' eigenvalue is then chosen as the arithmetic mean of all entries in the list, that are close to the target. This gives a value that, at most, differs by the absolute tolerance of the target used. All elements in the arithmetic mean are then discarded, and the process is repeated until there are no elements left in the list.
	
	Two considerations need to be taken when using this kind of algorithm: The relative and absolute tolerance. We need to make sure that the relative tolerance is larger than the convergence criterion. And we need to make sure that the absolute tolerance is large enough to make the algorithm not sensitive to permutations in the list:
	
	Say we have 3 eigenvalues in ascending order, each with a difference just smaller than the absolute tolerance, then the algorithm will return 2 different eigenvalues, whilst if the middle element is first, then only one eigenvalue is returned. The absolute tolerance thus needs to be large enough to accommodate this. An conservative estimate is to use the relative tolerance times the magnitude of the largest eigenvalue.
	
	Using this method we find the eigenvalues in table \ref{tab:K_eigs}:
	
	\begin{table}[H]
		\centering
		\begin{tabular}{l}
			Eigenvalues \\
			\hline
			13.892607255226826\\
			86.88021641811008 \\
			286.53330651449346\\
			1132.2165680659618\\
			1799.805890216019 \\
			5560.881197518796 \\
			11485.212833611125\\
			13338.622299303433\\
			22590.19851960052 \\
			32779.07108357938 \\
			36152.3699745374  \\
			50430.02765418719 \\
			52766.28875771519 \\
			93999.61412883797 \\
			151362.6664880097
		\end{tabular}
		\caption{Eigenvalues for the $ \V{K} $ matrix, found with a combination of inverse iteration and Rayleigh iteration.}
		\label{tab:K_eigs}
	\end{table}
	
	This is, of course, a highly inefficient method for finding distinct eigenvalues, since it necessitates computing many more eigenvalues than is actually needed. A better solution would be to implement some other algorithm which computes all eigenvalues simultaneously.
	
	Next we form the matrix $ \Vg{\Lambda}_1 = \V{U}\inverse \V{K} \V{U} $ and compare this to a proper diagonal matrix of the eigenvalues $ \Vg{\Lambda}_2 $ (using \texttt{np.diag} on a list of all the eigenvalues) by calculating the max-norm of the residual:
	\begin{equation}\label{key}
		|| \Vg{\Lambda}_1 - \Vg{\Lambda}_2 ||_{\infty} = 8.268 \D 10^{-11}
	\end{equation}
	Which means $ \Vg{\Lambda}_1 $ is not entirely diagonal - however, all non diagonal elements are on the order $ 10^{-11} $ or lower, attributable to truncation error in terminating the iterative algorithm and rounding error in the floating point arithmetic.
	
	
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\linewidth]{../nodes.pdf}
		\caption{Nodes of all 15 wavefunctions.}
		\label{fig:nodes}
	\end{figure}
	
	
\end{document}