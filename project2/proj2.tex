\documentclass[a4paper,10pt]{article}

\usepackage{Nikolai}
\usepackage[margin=0.75in]{geometry}

\title{Scientific Computing Project 2}
\author{Nikolai Plambech Nielsen\\LPK331}
\date{\today}


\begin{document}
	\maketitle
	\section*{A}
	The code is seen in file \texttt{f2.py}. Using the function on $ \textbf{K} $ gives the Gershgorin disks seen in table \ref{tab:disks}
	\begin{table}[H]
		\centering
		\begin{tabular}{r|r}
			Center & Radius \\
			\hline
			129292.2192 & 42231.6467 \\
			103041.4394 & 56927.5189 \\
			64967.5787 & 31160.6709 \\
			43612.4119 & 18532.3487 \\
			36273.7515 & 7870.5161 \\
			37990.0994 & 17854.7026 \\
			24166.9711 & 15414.7167 \\
			11651.1587 & 2512.0414 \\
			13865.0805 & 5502.9695 \\
			5600.5477 & 1195.2833 \\
			1173.0388 & 341.9902 \\
			1760.7714 & 401.2147 \\
			288.4231 & 71.4173 \\
			86.8969 & 2.5403 \\
			13.8933 & 0.2591
		\end{tabular}
	\caption{The Gershgorin disks for the matrix $ \textbf{K} $.}
	\label{tab:disks}
	\end{table}
	
	\section*{B}
	The convergence criterion chosen for the power iteration is
	\begin{equation}\label{key}
		\text{abs}\pp{\frac{\lambda_{n} - \lambda_{n-1}}{\lambda_{n-1}}} < \varepsilon
	\end{equation}
	where by default $ \varepsilon = 1\D 10^{-6} $, and the eigenvalue $ \lambda $ is calculated with the Rayleigh quotient. The eigenvalues, number of iterations before convergence and the Rayleigh residual of the example matrices and $ \V{K} $ are tabulated below in table \ref{tab:power_iter}
	\begin{table}[H]
		\centering
		\begin{tabular}{c|c|c|c}
			Matrix & Eigenvalue & Rayleigh Residual & Iterations \\
			\hline
			$ \textbf{A}_1 $ &  3.999999 & 0.0036 & 10\\
			$ \textbf{A}_2 $ &  3.999998 & 0.0022 & 7\\
			$ \textbf{A}_3 $ & 12.298959 & $ 8.9 \D 10^{-6} $ & 14\\
			$ \textbf{A}_4 $ & 16.116844 & $ 1.1 \D 10^{-6} $ & 6\\
			$ \textbf{A}_5 $ & 68.642081 & $ 1.3\D10^-6 $ & 6\\
			$ \textbf{A}_6 $ &  1.999999 & 0.0011 & 4\\
			$ \V{K} $ & 151362.67 & 0.016 & 31
		\end{tabular}
		\caption{Largest eigenvalue of the example matrices and $ \V{K} $ using power iteration, with Rayleigh residual and number of iterations until convergence shown.}
		\label{tab:power_iter}
	\end{table}
	For each of the tables a random starting vector is chosen with \texttt{np.random.uniform}
	
	\section*{C}
	The same convergence criterion is used for Rayleigh iteration as for power iteration. If we use the LU-solver for computing the solution, the method will not be robust for singular matrices. The shifts may make the matrix non-singular, allowing us to use the algorithm, but it probably should not be relied upon. As such we choose the QR-solver. The result are shown in table \ref{tab:rayleigh_iter}.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{c|c|c|c}
			Matrix & Eigenvalue & Rayleigh Residual & Iterations \\
			\hline
			$ \textbf{A}_1 $ & 4.0 & $ 1.5 \D 10^{-9} $ & 3 \\
			$ \textbf{A}_2 $ & 4.0 & $ 2.9 \D 10^{-9} $ & 2 \\
			$ \textbf{A}_3 $ & 12.298958 & $ 1.2 \D 10^{-14} $ & 5 \\
			$ \textbf{A}_4 $ & 16.116844 & $ 3.2 \D 10^{-11} $ & 4 \\
			$ \textbf{A}_5 $ & 68.642081 & $ 1.7 \D 10^{-13} $ & 10 \\
			$ \textbf{A}_6 $ &  2.000000 & $ 5.1 \D 10^{-12} $ & 2
		\end{tabular}
		\caption{Largest eigenvalue of the example matrices using Rayleigh iteration, with Rayleigh residual and number of iterations until convergence also shown.}
		\label{tab:rayleigh_iter}
	\end{table}
	
	
	\section*{D}
	To calculate multiple eigenvalues of $ \V{K} $ we need to use one or more of the algorithms above several times, with different shifts. In general the Rayleigh iteration uses fewer iterations before convergence is achieved, so this is the employed algorithm. We further use 5 inverse iterations with a constant shift to hone in on a specific eigenvalue, to counteract the fluctuations introduced by using a random starting vector.
	
	Next we localize the eigenvalues of $ \V{K} $ as done in problem A. We use the ends and centre of each Gershgorin disc as shifts, yielding 44 eigenvalues. A single eigenvalue did not converge - using the first element of $ \V{K} $ as a shift means we perform QR-factorization on a matrix with a 0 in its first diagonal element. This yields a singular $ \V{R} $ matrix in the QR-factorization, stopping the backwards substitution.
	
	Then we just need to weed out the eigenvalues which are not unique, which should then give us all the eigenvalues with different values. The problem is that using strict numerical equality to evaluate uniqueness necessarily ignores any truncation error of the algorithm. To account for this we use the function \texttt{np.isclose} with a relative tolerance of $ 10^{-5} $ and an absolute tolerance of $ 10^{-8} $.
	
	The code (\texttt{find\_unique} in the file \texttt{f2.py}) runs a loop where the first entry is chosen, and used as the target of the isclose function. The ``true'' eigenvalue is then chosen as the arithmetic mean of all entries in the list, that are close to the target. This gives a value that, at most, differs by the absolute tolerance of the target used. All elements in the arithmetic mean are then discarded, and the process is repeated until there are no elements left in the list.
	
	Two considerations need to be taken when using this kind of algorithm: The relative and absolute tolerance. We need to make sure that the relative tolerance is larger than the convergence criterion. And we need to make sure that the absolute tolerance is large enough to make the algorithm not sensitive to permutations in the list:
	
	Say we have 3 eigenvalues in ascending order, each with a difference just smaller than the absolute tolerance, then the algorithm will return 2 different eigenvalues, whilst if the middle element is first, then only one eigenvalue is returned. The absolute tolerance thus needs to be large enough to accommodate this. An conservative estimate is to use the relative tolerance times the magnitude of the largest eigenvalue.
	
	Using this method we find the eigenvalues in table \ref{tab:K_eigs}:
	
	\begin{table}[H]
		\centering
		\begin{tabular}{l}
			Eigenvalues \\
			\hline
			13.8926     \\
			86.8802     \\
			286.5333    \\
			1132.2166   \\
			1799.8059   \\
			5560.8812   \\
			11485.2128  \\
			13338.6223  \\
			22590.1985  \\
			32779.0711  \\
			36152.3700  \\
			50430.0277  \\
			52766.2888  \\
			93999.6141  \\
			151362.6665
		\end{tabular}
		\caption{Eigenvalues for the $ \V{K} $ matrix, found with a combination of inverse iteration and Rayleigh iteration.}
		\label{tab:K_eigs}
	\end{table}
	
	This is, of course, a highly inefficient method for finding distinct eigenvalues, since it necessitates computing many more eigenvalues than is actually needed. A better solution would be to implement some other algorithm which computes all eigenvalues simultaneously.
	
	Next we form the matrix $ \Vg{\Lambda}_1 = \V{U}\inverse \V{K} \V{U} $ and compare this to a proper diagonal matrix of the eigenvalues $ \Vg{\Lambda}_2 $ (using \texttt{np.diag} on a list of all the eigenvalues) by calculating the max-norm of the residual:
	\begin{equation}\label{key}
		|| \Vg{\Lambda}_1 - \Vg{\Lambda}_2 ||_{\infty} = 1.587 \D 10^{-6}
	\end{equation}
	Which means $ \Vg{\Lambda}_1 $ is not entirely diagonal. We note that the max norm is on the order of the stopping criteria parameter $ \epsilon $. Changing this to $ 10^{-10} $ and calculating the max norm again gives
	\begin{equation}\label{key}
	|| \Vg{\Lambda}_1 - \Vg{\Lambda}_2 ||_{\infty} = 1.658 \D 10^{-10}, \quad \epsilon=10^{-10}
	\end{equation}
	As such it seems the discrepancy between the two matrices is largely due to the truncation error introduced by stopping the iterative algorithm.
	
	Lastly we use the \texttt{show\_all\_wavefunction\_nodes} function with the found eigenvectors and eigenvalues. The result is shown in \ref{fig:nodes}.
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\linewidth]{nodes.pdf}
		\caption{Nodes of all 15 wavefunctions.}
		\label{fig:nodes}
	\end{figure}
	
	
\end{document}